#!/usr/bin/env python3
"""
è‡ªåŠ¨åŒ–è®­ç»ƒè°ƒåº¦å™¨

ä¸“ä¸ºç”Ÿäº§ç¯å¢ƒè®¾è®¡çš„ç®€åŒ–ç‰ˆæœ¬ï¼Œç‰¹ç‚¹ï¼š
- ä¸éœ€è¦ç‰ˆæœ¬æ§åˆ¶ï¼Œåªä¿ç•™æ¯æ—¥æœ€æ–°æ¨¡å‹
- å®Œå…¨è‡ªåŠ¨åŒ–ï¼Œæ— éœ€äººå·¥å¹²é¢„
- åŸºäºç°æœ‰è®­ç»ƒç®¡é“ï¼Œç›´æ¥è°ƒç”¨scripts/run_pipeline.py
- æ™ºèƒ½è®­ç»ƒå†³ç­–å’Œç›‘æ§
"""

import os
import sys
import logging
import subprocess
from datetime import datetime, timedelta
from typing import Dict, List, Optional, Tuple
from dataclasses import dataclass
import json

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„
sys.path.append(os.path.dirname(os.path.dirname(os.path.dirname(__file__))))

from scripts.run_pipeline import train_cities, show_model_status, cleanup_old_models
from ml.src.control import get_supported_cities
from ml.src.data_loader import load_data_from_mysql


@dataclass
class SimpleTrainingResult:
    """è®­ç»ƒç»“æœ"""
    timestamp: str
    total_cities: int
    successful_cities: int
    failed_cities: int
    skipped_cities: int
    execution_time: float
    successful_city_list: List[str]
    failed_city_list: List[str]
    skipped_city_list: List[str]


class SimpleAutoTrainingScheduler:
    """è‡ªåŠ¨åŒ–è®­ç»ƒè°ƒåº¦å™¨"""
    
    def __init__(self, log_dir: str = None):
        """
        åˆå§‹åŒ–è®­ç»ƒè°ƒåº¦å™¨
        
        Args:
            log_dir (str): æ—¥å¿—ç›®å½•ï¼Œé»˜è®¤ä¸º logs/auto_training
        """
        self.log_dir = log_dir or os.path.join(os.getcwd(), 'logs', 'auto_training')
        self.logger = self._setup_logger()
        
    def _setup_logger(self) -> logging.Logger:
        """è®¾ç½®æ—¥å¿—è®°å½•å™¨"""
        logger = logging.getLogger('SimpleAutoTrainingScheduler')
        logger.setLevel(logging.INFO)
        
        # é¿å…é‡å¤æ·»åŠ handler
        if logger.handlers:
            return logger
        
        # åˆ›å»ºæ—¥å¿—ç›®å½•
        os.makedirs(self.log_dir, exist_ok=True)
        
        # æ–‡ä»¶handler - æŒ‰æ—¥æœŸå‘½å
        today = datetime.now().strftime('%Y%m%d')
        log_file = os.path.join(self.log_dir, f'auto_training_{today}.log')
        
        file_handler = logging.FileHandler(log_file, encoding='utf-8')
        file_handler.setLevel(logging.INFO)
        
        # æ ¼å¼åŒ–å™¨
        formatter = logging.Formatter(
            '%(asctime)s - %(name)s - %(levelname)s - %(message)s',
            datefmt='%Y-%m-%d %H:%M:%S'
        )
        file_handler.setFormatter(formatter)

        logger.addHandler(file_handler)
        
        return logger
    
    def check_data_freshness(self, days_threshold: int = 3, force_override: bool = False) -> Tuple[List[str], List[str]]:
        """
        æ£€æŸ¥æ•°æ®å¯ç”¨æ€§å’Œè®­ç»ƒéœ€æ±‚ï¼Œç¡®å®šå“ªäº›åŸå¸‚éœ€è¦è®­ç»ƒ
        
        Args:
            days_threshold (int): æ•°æ®å¯ç”¨æ€§é˜ˆå€¼ï¼ˆå¤©æ•°ï¼‰ï¼Œé»˜è®¤3å¤©å†…æœ‰æ•°æ®å³å¯
            force_override (bool): æ˜¯å¦å¼ºåˆ¶è¦†ç›–æ¨¡å¼ï¼Œè·³è¿‡å·²è®­ç»ƒæ£€æŸ¥
            
        Returns:
            Tuple[List[str], List[str]]: (éœ€è¦è®­ç»ƒçš„åŸå¸‚, è·³è¿‡çš„åŸå¸‚)
        """
        cities = get_supported_cities()
        cities_to_train = []
        cities_to_skip = []
        
        # æ•°æ®å¯ç”¨æ€§é˜ˆå€¼ï¼šæœ€è¿‘Nå¤©å†…æœ‰æ•°æ®å³å¯
        data_cutoff_time = datetime.now() - timedelta(days=days_threshold)
        today_str = datetime.now().strftime("%Y%m%d")
        
        for city in cities:
            try:
                # 1. æ£€æŸ¥ä»Šå¤©æ˜¯å¦å·²ç»è®­ç»ƒè¿‡æ¨¡å‹ï¼ˆå¼ºåˆ¶è¦†ç›–æ—¶è·³è¿‡æ­¤æ£€æŸ¥ï¼‰
                if not force_override:
                    from scripts.run_pipeline import is_model_trained_today
                    if is_model_trained_today(city):
                        cities_to_skip.append(city)
                        self.logger.info(f"{city}: ä»Šæ—¥æ¨¡å‹å·²å­˜åœ¨ï¼Œè·³è¿‡è®­ç»ƒ")
                        continue
                else:
                    self.logger.info(f"{city}: å¼ºåˆ¶è¦†ç›–æ¨¡å¼ï¼Œå¿½ç•¥å·²è®­ç»ƒæ£€æŸ¥")
                
                # 2. æ£€æŸ¥æ•°æ®å¯ç”¨æ€§
                df = load_data_from_mysql(city)
                if df.empty:
                    cities_to_skip.append(city)
                    self.logger.warning(f"{city}: æ— å¯ç”¨æ•°æ®")
                    continue
                
                # 3. æ£€æŸ¥æ•°æ®é‡æ˜¯å¦è¶³å¤Ÿ
                if len(df) < 100:
                    cities_to_skip.append(city)
                    self.logger.warning(f"{city}: æ•°æ®é‡ä¸è¶³ ({len(df)}æ¡)")
                    continue
                
                # 4. æ£€æŸ¥æ•°æ®æ˜¯å¦å¤ªé™ˆæ—§ï¼ˆè¶…è¿‡é˜ˆå€¼å¤©æ•°ï¼‰
                latest_time = df['observation_time'].max()
                if latest_time < data_cutoff_time:
                    cities_to_skip.append(city)
                    self.logger.warning(f"{city}: æ•°æ®è¿‡äºé™ˆæ—§ (æœ€æ–°: {latest_time}, é˜ˆå€¼: {days_threshold}å¤©)")
                    continue
                
                # 5. æ‰€æœ‰æ£€æŸ¥é€šè¿‡ï¼Œå¯ä»¥è®­ç»ƒ
                cities_to_train.append(city)
                days_old = (datetime.now() - latest_time).days
                self.logger.info(f"{city}: æ•°æ®æ£€æŸ¥é€šè¿‡ (æœ€æ–°: {latest_time}, {days_old}å¤©å‰, å…±{len(df)}æ¡)")
                
            except Exception as e:
                cities_to_skip.append(city)
                self.logger.error(f"{city}: æ•°æ®æ£€æŸ¥å¤±è´¥ - {str(e)}")
        
        self.logger.info(f"æ•°æ®æ£€æŸ¥å®Œæˆ: éœ€è®­ç»ƒ{len(cities_to_train)}ä¸ªåŸå¸‚, è·³è¿‡{len(cities_to_skip)}ä¸ªåŸå¸‚")
        return cities_to_train, cities_to_skip
    
    def run_daily_training(self, force_override: bool = False) -> SimpleTrainingResult:
        """
        æ‰§è¡Œæ¯æ—¥è‡ªåŠ¨è®­ç»ƒ
        
        Args:
            force_override (bool): æ˜¯å¦å¼ºåˆ¶è¦†ç›–æ¨¡å¼ï¼Œåˆ é™¤ç°æœ‰æ¨¡å‹å’Œç¼“å­˜é‡æ–°è®­ç»ƒ
        
        Returns:
            SimpleTrainingResult: è®­ç»ƒç»“æœ
        """
        start_time = datetime.now()
        self.logger.info("=" * 60)
        if force_override:
            self.logger.info(f"å¼€å§‹å¼ºåˆ¶è¦†ç›–è®­ç»ƒ - {start_time.strftime('%Y-%m-%d %H:%M:%S')}")
        else:
            self.logger.info(f"å¼€å§‹æ¯æ—¥è‡ªåŠ¨æ¨¡å‹è®­ç»ƒ - {start_time.strftime('%Y-%m-%d %H:%M:%S')}")
        self.logger.info("=" * 60)
        
        try:
            
            # 2. æ£€æŸ¥æ•°æ®æ–°é²œåº¦
            self.logger.info("\nğŸ” æ£€æŸ¥æ•°æ®æ–°é²œåº¦...")
            cities_to_train, cities_to_skip = self.check_data_freshness(force_override=force_override)
            
            if not cities_to_train:
                self.logger.info("æ‰€æœ‰åŸå¸‚éƒ½å·²è·³è¿‡ï¼Œæ— éœ€è®­ç»ƒ")
                end_time = datetime.now()
                return SimpleTrainingResult(
                    timestamp=start_time.isoformat(),
                    total_cities=len(get_supported_cities()),
                    successful_cities=0,
                    failed_cities=0,
                    skipped_cities=len(cities_to_skip),
                    execution_time=(end_time - start_time).total_seconds(),
                    successful_city_list=[],
                    failed_city_list=[],
                    skipped_city_list=cities_to_skip
                )
            
            # 3. æ‰§è¡Œè®­ç»ƒï¼ˆåªè®­ç»ƒéœ€è¦è®­ç»ƒçš„åŸå¸‚ï¼‰
            self.logger.info(f"\nğŸš€ å¼€å§‹è®­ç»ƒ {len(cities_to_train)} ä¸ªåŸå¸‚...")
            training_results = train_cities(cities_to_train, force_override=force_override)
            
            # 4. è§£æè®­ç»ƒç»“æœï¼ˆåˆå¹¶é¢„æ£€æŸ¥çš„è·³è¿‡åŸå¸‚ï¼‰
            successful_cities = training_results.get('successful', [])
            failed_cities = training_results.get('failed', [])
            skipped_cities_from_training = training_results.get('skipped', [])
            
            # åˆå¹¶ä¸¤ç§è·³è¿‡çš„åŸå¸‚ï¼šé¢„æ£€æŸ¥è·³è¿‡ + è®­ç»ƒæ—¶è·³è¿‡
            all_skipped_cities = cities_to_skip + skipped_cities_from_training
            
            # 5. æ¸…ç†æ—§æ¨¡å‹
            self.logger.info("\nğŸ§¹ æ¸…ç†æ—§æ¨¡å‹æ–‡ä»¶...")
            cleanup_old_models(days_to_keep=7)
            
            # 6. é¢„è®¡ç®—ä»Šæ—¥é¢„æµ‹æ•°æ®
            if successful_cities:
                self.logger.info("\nğŸ”® å¼€å§‹é¢„è®¡ç®—ä»Šæ—¥é¢„æµ‹æ•°æ®...")
                precompute_result = self._precompute_daily_predictions(successful_cities)
                self.logger.info(f"é¢„è®¡ç®—å®Œæˆ: æˆåŠŸ{precompute_result['successful']}ä¸ªåŸå¸‚, å¤±è´¥{precompute_result['failed']}ä¸ªåŸå¸‚")
            elif force_override:
                # å¼ºåˆ¶è¦†ç›–æ¨¡å¼ï¼šå³ä½¿æ²¡æœ‰æ–°è®­ç»ƒæ¨¡å‹ï¼Œä¹Ÿè¦é‡æ–°é¢„è®¡ç®—æ‰€æœ‰åŸå¸‚
                self.logger.info("\nğŸ”® å¼ºåˆ¶è¦†ç›–æ¨¡å¼ï¼šé‡æ–°é¢„è®¡ç®—æ‰€æœ‰åŸå¸‚...")
                all_cities = get_supported_cities()
                precompute_result = self._precompute_daily_predictions(all_cities)
                self.logger.info(f"å¼ºåˆ¶é¢„è®¡ç®—å®Œæˆ: æˆåŠŸ{precompute_result['successful']}ä¸ªåŸå¸‚, å¤±è´¥{precompute_result['failed']}ä¸ªåŸå¸‚")
            else:
                self.logger.info("\nâ­ï¸ è·³è¿‡é¢„è®¡ç®—ï¼ˆæ— æ–°è®­ç»ƒæ¨¡å‹ï¼‰")
            
            # 7. è®­ç»ƒå®Œæˆ
            
            # 8. ç”Ÿæˆç»“æœ
            end_time = datetime.now()
            execution_time = (end_time - start_time).total_seconds()
            
            result = SimpleTrainingResult(
                timestamp=start_time.isoformat(),
                total_cities=len(get_supported_cities()),
                successful_cities=len(successful_cities),
                failed_cities=len(failed_cities),
                skipped_cities=len(all_skipped_cities),
                execution_time=execution_time,
                successful_city_list=successful_cities,
                failed_city_list=failed_cities,
                skipped_city_list=all_skipped_cities
            )
            
            # 8. è®°å½•è®­ç»ƒæŠ¥å‘Š
            self._save_training_report(result)
            
            # 9. è¾“å‡ºæ€»ç»“
            self.logger.info("=" * 60)
            self.logger.info("ğŸ“‹ æ¯æ—¥è®­ç»ƒå®Œæˆæ€»ç»“:")
            self.logger.info(f"   æ‰§è¡Œæ—¶é—´: {execution_time:.1f}ç§’")
            self.logger.info(f"   æ€»åŸå¸‚æ•°: {result.total_cities}")
            self.logger.info(f"   è®­ç»ƒæˆåŠŸ: {result.successful_cities}")
            self.logger.info(f"   è®­ç»ƒå¤±è´¥: {result.failed_cities}")
            self.logger.info(f"   è·³è¿‡è®­ç»ƒ: {result.skipped_cities}")
            
            if result.successful_cities > 0:
                self.logger.info(f"   æˆåŠŸåŸå¸‚: {', '.join(result.successful_city_list)}")
            if result.failed_cities > 0:
                self.logger.info(f"   å¤±è´¥åŸå¸‚: {', '.join(result.failed_city_list)}")
            
            self.logger.info("=" * 60)
            
            return result
            
        except Exception as e:
            end_time = datetime.now()
            self.logger.error(f"æ¯æ—¥è®­ç»ƒæ‰§è¡Œå¤±è´¥: {str(e)}")
            
            # è¿”å›å¤±è´¥ç»“æœ
            return SimpleTrainingResult(
                timestamp=start_time.isoformat(),
                total_cities=len(get_supported_cities()),
                successful_cities=0,
                failed_cities=len(get_supported_cities()),
                skipped_cities=0,
                execution_time=(end_time - start_time).total_seconds(),
                successful_city_list=[],
                failed_city_list=get_supported_cities(),
                skipped_city_list=[]
            )
    
    def _save_training_report(self, result: SimpleTrainingResult):
        """ä¿å­˜è®­ç»ƒæŠ¥å‘Šåˆ°JSONæ–‡ä»¶"""
        try:
            report_file = os.path.join(
                self.log_dir, 
                f"training_report_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
            )
            
            report_data = {
                'timestamp': result.timestamp,
                'summary': {
                    'total_cities': result.total_cities,
                    'successful_cities': result.successful_cities,
                    'failed_cities': result.failed_cities,
                    'skipped_cities': result.skipped_cities,
                    'execution_time_seconds': result.execution_time
                },
                'details': {
                    'successful_city_list': result.successful_city_list,
                    'failed_city_list': result.failed_city_list,
                    'skipped_city_list': result.skipped_city_list
                }
            }
            
            with open(report_file, 'w', encoding='utf-8') as f:
                json.dump(report_data, f, indent=2, ensure_ascii=False)
            
            self.logger.info(f"è®­ç»ƒæŠ¥å‘Šå·²ä¿å­˜: {report_file}")
            
        except Exception as e:
            self.logger.error(f"ä¿å­˜è®­ç»ƒæŠ¥å‘Šå¤±è´¥: {str(e)}")
    
    def health_check(self) -> Dict[str, bool]:
        """
        ç³»ç»Ÿå¥åº·æ£€æŸ¥
        
        Returns:
            Dict[str, bool]: å„é¡¹æ£€æŸ¥ç»“æœ
        """
        checks = {}
        
        try:
            # æ£€æŸ¥æ•°æ®åº“è¿æ¥
            cities = get_supported_cities()
            checks['database_connection'] = len(cities) > 0
            
            # æ£€æŸ¥æ¨¡å‹ç›®å½•
            from config.paths import DAILY_MODELS_DIR, LATEST_MODELS_DIR
            checks['model_directories'] = (
                os.path.exists(DAILY_MODELS_DIR) and 
                os.path.exists(LATEST_MODELS_DIR)
            )
            
            # æ£€æŸ¥æ—¥å¿—ç›®å½•
            checks['log_directory'] = os.path.exists(self.log_dir)
            
            # æ£€æŸ¥æ•°æ®å¯ç”¨æ€§
            data_available = 0
            for city in cities[:3]:  # åªæ£€æŸ¥å‰3ä¸ªåŸå¸‚
                try:
                    df = load_data_from_mysql(city)
                    if not df.empty:
                        data_available += 1
                except:
                    pass
            checks['data_availability'] = data_available > 0
            
        except Exception as e:
            self.logger.error(f"å¥åº·æ£€æŸ¥å¤±è´¥: {str(e)}")
            checks['system_error'] = False
        
        return checks
    
    def _precompute_daily_predictions(self, cities: List[str]) -> Dict[str, int]:
        """
        é¢„è®¡ç®—æ‰€æœ‰åŸå¸‚çš„24å°æ—¶é¢„æµ‹æ•°æ®
        
        Args:
            cities (List[str]): éœ€è¦é¢„è®¡ç®—çš„åŸå¸‚åˆ—è¡¨
            
        Returns:
            Dict[str, int]: é¢„è®¡ç®—ç»“æœç»Ÿè®¡ {'successful': int, 'failed': int}
        """
        from ml.src.predict import predict_for_web_api
        import pandas as pd
        
        predictions_cache = {}
        successful_count = 0
        failed_count = 0
        
        for city in cities:
            try:
                self.logger.info(f"  æ­£åœ¨é¢„è®¡ç®— {city}...")
                
                # æ‰§è¡Œé¢„æµ‹
                predictions_df = predict_for_web_api(city, steps=24)
                
                # æ ¼å¼åŒ–é¢„æµ‹æ•°æ®ä¸ºAPIéœ€è¦çš„æ ¼å¼
                formatted_data = self._format_predictions_for_api(predictions_df)
                predictions_cache[city] = formatted_data
                
                successful_count += 1
                self.logger.info(f"  âœ… {city} é¢„æµ‹æ•°æ®å·²ç”Ÿæˆ (24å°æ—¶)")
                
            except Exception as e:
                failed_count += 1
                self.logger.error(f"  âŒ {city} é¢„æµ‹å¤±è´¥: {str(e)}")
        
        # ä¿å­˜é¢„æµ‹ç¼“å­˜åˆ°æ–‡ä»¶
        if predictions_cache:
            self._save_predictions_cache(predictions_cache)
            self.logger.info(f"ğŸ“ é¢„æµ‹ç¼“å­˜å·²ä¿å­˜ ({len(predictions_cache)} ä¸ªåŸå¸‚)")
        
        return {
            'successful': successful_count,
            'failed': failed_count
        }
    
    def _format_predictions_for_api(self, predictions_df) -> Dict:
        """
        å°†é¢„æµ‹DataFrameæ ¼å¼åŒ–ä¸ºAPIè¿”å›çš„JSONæ ¼å¼
        
        Args:
            predictions_df (pd.DataFrame): é¢„æµ‹ç»“æœDataFrame
            
        Returns:
            Dict: APIæ ¼å¼çš„é¢„æµ‹æ•°æ®
        """
        import pandas as pd
        
        if predictions_df is None or predictions_df.empty:
            raise Exception("é¢„æµ‹æ•°æ®ä¸ºç©º")
        
        # æå–24å°æ—¶é¢„æµ‹æ•°æ®
        times = [pd.to_datetime(t).strftime("%H:%M") for t in predictions_df['observation_time'].tolist()[:24]]
        values = predictions_df['prediction'].tolist()[:24]
        low = predictions_df['lower_bound'].tolist()[:24]
        high = predictions_df['upper_bound'].tolist()[:24]
        
        current_value = values[0] if values else 0
        avg_value = sum(values) / len(values) if values else 0
        
        # ç”ŸæˆAPIæ ¼å¼æ•°æ®
        formatted_data = {
            "updateTime": datetime.now().strftime("%Y-%m-%d %H:%M"),
            "currentValue": round(current_value, 1),
            "avgValue": round(avg_value, 1),
            "times": times,
            "values": [round(v, 1) for v in values],
            "low": [round(l, 1) for l in low],
            "high": [round(h, 1) for h in high],
            "cached": True,  # æ ‡è®°ä¸ºç¼“å­˜æ•°æ®
            "cache_time": datetime.now().isoformat()
        }
        
        return formatted_data
    
    def _save_predictions_cache(self, predictions_cache: Dict):
        """
        ä¿å­˜é¢„æµ‹ç¼“å­˜åˆ°æ–‡ä»¶
        
        Args:
            predictions_cache (Dict): é¢„æµ‹ç¼“å­˜æ•°æ®
        """
        try:
            # ç¡®ä¿ç¼“å­˜ç›®å½•å­˜åœ¨
            cache_dir = os.path.join(os.getcwd(), 'data', 'predictions_cache')
            os.makedirs(cache_dir, exist_ok=True)
            
            # ç”Ÿæˆæ–‡ä»¶å
            today_str = datetime.now().strftime('%Y%m%d')
            cache_file = os.path.join(cache_dir, f'daily_predictions_{today_str}.json')
            latest_cache_file = os.path.join(cache_dir, 'latest_predictions.json')
            
            # æ·»åŠ å…ƒæ•°æ®
            cache_data = {
                'generated_at': datetime.now().isoformat(),
                'date': today_str,
                'cities_count': len(predictions_cache),
                'predictions': predictions_cache
            }
            
            # ä¿å­˜å¸¦æ—¥æœŸçš„ç¼“å­˜æ–‡ä»¶
            with open(cache_file, 'w', encoding='utf-8') as f:
                json.dump(cache_data, f, indent=2, ensure_ascii=False)
            
            # ä¿å­˜æœ€æ–°ç¼“å­˜æ–‡ä»¶ï¼ˆè¦†ç›–ï¼‰
            with open(latest_cache_file, 'w', encoding='utf-8') as f:
                json.dump(cache_data, f, indent=2, ensure_ascii=False)
            
            self.logger.info(f"ç¼“å­˜æ–‡ä»¶å·²ä¿å­˜:")
            self.logger.info(f"  - æ—¥æœŸç‰ˆæœ¬: {cache_file}")
            self.logger.info(f"  - æœ€æ–°ç‰ˆæœ¬: {latest_cache_file}")
            
        except Exception as e:
            self.logger.error(f"ä¿å­˜é¢„æµ‹ç¼“å­˜å¤±è´¥: {str(e)}")
    
    def load_predictions_cache(self, date_str: str = None) -> Optional[Dict]:
        """
        åŠ è½½é¢„æµ‹ç¼“å­˜æ•°æ®
        
        Args:
            date_str (str): æ—¥æœŸå­—ç¬¦ä¸² (YYYYMMDD)ï¼Œé»˜è®¤ä¸ºä»Šå¤©
            
        Returns:
            Optional[Dict]: ç¼“å­˜æ•°æ®ï¼Œå¦‚æœä¸å­˜åœ¨è¿”å›None
        """
        try:
            cache_dir = os.path.join(os.getcwd(), 'data', 'predictions_cache')
            
            if date_str is None:
                # åŠ è½½æœ€æ–°ç¼“å­˜
                cache_file = os.path.join(cache_dir, 'latest_predictions.json')
            else:
                # åŠ è½½æŒ‡å®šæ—¥æœŸç¼“å­˜
                cache_file = os.path.join(cache_dir, f'daily_predictions_{date_str}.json')
            
            if not os.path.exists(cache_file):
                self.logger.warning(f"ç¼“å­˜æ–‡ä»¶ä¸å­˜åœ¨: {cache_file}")
                return None
            
            with open(cache_file, 'r', encoding='utf-8') as f:
                cache_data = json.load(f)
            
            self.logger.info(f"å·²åŠ è½½é¢„æµ‹ç¼“å­˜: {cache_file}")
            return cache_data
            
        except Exception as e:
            self.logger.error(f"åŠ è½½é¢„æµ‹ç¼“å­˜å¤±è´¥: {str(e)}")
            return None


def main():
    """å‘½ä»¤è¡Œå…¥å£"""
    import sys
    
    # æ£€æŸ¥å‘½ä»¤è¡Œå‚æ•°
    if len(sys.argv) > 1:
        command = sys.argv[1]
        
        scheduler = SimpleAutoTrainingScheduler()
        
        if command == 'health':
            # å¥åº·æ£€æŸ¥
            checks = scheduler.health_check()
            print("ğŸ¥ ç³»ç»Ÿå¥åº·æ£€æŸ¥:")
            for check_name, status in checks.items():
                status_icon = "âœ…" if status else "âŒ"
                print(f"  {status_icon} {check_name}: {'é€šè¿‡' if status else 'å¤±è´¥'}")
            
            # å¦‚æœæœ‰æ£€æŸ¥å¤±è´¥ï¼Œé€€å‡ºç ä¸º1
            if not all(checks.values()):
                sys.exit(1)
                
        elif command == 'run':
            # æ‰§è¡Œè®­ç»ƒ
            result = scheduler.run_daily_training()
            
            # å¦‚æœæœ‰å¤±è´¥çš„åŸå¸‚ï¼Œé€€å‡ºç ä¸º1
            if result.failed_cities > 0:
                sys.exit(1)
        
        else:
            print(f"æœªçŸ¥å‘½ä»¤: {command}")
            print("å¯ç”¨å‘½ä»¤: health, run")
            sys.exit(1)
    
    else:
        # é»˜è®¤æ‰§è¡Œè®­ç»ƒ
        scheduler = SimpleAutoTrainingScheduler()
        result = scheduler.run_daily_training()
        
        # å¦‚æœæœ‰å¤±è´¥çš„åŸå¸‚ï¼Œé€€å‡ºç ä¸º1
        if result.failed_cities > 0:
            sys.exit(1)


if __name__ == '__main__':
    main()